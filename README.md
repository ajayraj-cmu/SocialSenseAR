# SocialSenseAR

Real-time AR environment modifier with voice control, using SAM (Segment Anything Model), Gemini Vision, and sensory modulation features.

## ğŸš€ Quick Start

### Main Application (Voice-Controlled)

```bash
python scripts/sam_gemini_voice.py
```

**Voice Commands:**
- Say **"hey vibe"** to start recording
- Say your command (e.g., "blur my face", "dim the ceiling")
- Say **"thanks"** to process

### Alternative Entry Point

```bash
python main.py
```

## ğŸ“ Project Structure

```
SocialSenseAR/
â”œâ”€â”€ main.py                 # Main entry point (perceptual modulation engine)
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ config/                 # Configuration files
â”œâ”€â”€ src/                     # Core source code
â”‚   â”œâ”€â”€ audio/              # Audio processing
â”‚   â”œâ”€â”€ capture/            # Video capture
â”‚   â”œâ”€â”€ core/               # Core contracts and types
â”‚   â”œâ”€â”€ depth/              # Depth estimation
â”‚   â”œâ”€â”€ intent/             # NLP and intent parsing
â”‚   â”œâ”€â”€ pipeline/           # Main pipeline orchestrator
â”‚   â”œâ”€â”€ safety/             # Safety layer and monitoring
â”‚   â”œâ”€â”€ segmentation/       # SAM segmentation
â”‚   â”œâ”€â”€ tracking/           # Object tracking
â”‚   â”œâ”€â”€ transforms/         # Visual transformations
â”‚   â””â”€â”€ voice/              # Voice command processing
â”œâ”€â”€ scripts/                # Standalone scripts and demos
â”‚   â”œâ”€â”€ sam_gemini_voice.py # Main voice-controlled app â­
â”‚   â”œâ”€â”€ sam_*.py            # Various SAM demo scripts
â”‚   â””â”€â”€ fast_*.py           # FastSAM scripts
â”œâ”€â”€ docs/                   # Documentation
â”‚   â”œâ”€â”€ README.md           # Main documentation
â”‚   â”œâ”€â”€ PIPELINE_DOCUMENTATION.md
â”‚   â”œâ”€â”€ FEEDBACK_LOOP_DOCUMENTATION.md
â”‚   â””â”€â”€ *.md                # Other documentation
â”œâ”€â”€ models/                 # Model weights
â”‚   â”œâ”€â”€ FastSAM-s.pt
â”‚   â”œâ”€â”€ yolov8*.pt
â”‚   â””â”€â”€ sam_*.pth
â”œâ”€â”€ assets/                 # Images, HTML, etc.
â”œâ”€â”€ logs/                   # Log files
â””â”€â”€ recordings/             # Audio/video recordings
```

## ğŸ¯ Features

- **Voice Control**: Wake word activation ("hey vibe" / "thanks")
- **Real-time Segmentation**: FastSAM for object segmentation
- **Smart Labeling**: Gemini Vision API for open-vocabulary detection
- **Sensory Modulation**: Blur, brightness, color, motion dampening
- **Persistent Tracking**: Masks track objects during movement
- **Clean View Mode**: Toggle between full view and effects-only

## ğŸ“‹ Requirements

```bash
pip install -r requirements.txt
```

Key dependencies:
- `ultralytics` (FastSAM, YOLO)
- `google-generativeai` (Gemini API)
- `speech_recognition` (Voice commands)
- `mediapipe` (Body part segmentation)
- `opencv-python` (Video processing)

## ğŸ”§ Configuration

Create a `.env` file:

```env
GEMINI_API_KEY=your-gemini-api-key
OVERSHOOT_API_KEY=your-overshoot-api-key  # Optional
```

## ğŸ“– Documentation

See `docs/` folder for detailed documentation:
- `PIPELINE_DOCUMENTATION.md` - Full pipeline architecture
- `FEEDBACK_LOOP_DOCUMENTATION.md` - Self-correction system
- `USAGE_GUIDE.md` - Usage instructions

## âŒ¨ï¸ Controls

- **V** - Toggle clean/full view
- **C** - Clear all effects
- **L** - List all detected labels
- **S** - Screenshot
- **Q** - Quit

## ğŸ¤ Contributing

See the main documentation in `docs/` for architecture details and contribution guidelines.

## ğŸ“„ License

MIT License

